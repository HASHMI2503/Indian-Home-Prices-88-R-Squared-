{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5244696,"sourceType":"datasetVersion","datasetId":3051697}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Analysis Summary\n\n1. **Data Import and Cleaning:**\n   - Imported the dataset and converted the 'number of bathrooms' column to an integer type.\n   - Dropped the 'id' column for simplicity.\n\n2. **PCA on Numeric Columns:**\n   - Applied Principal Component Analysis (PCA) to numeric columns, excluding 'date' and the output column ('price').\n\n3. **Two Experimental Paths:**\n   - **Path 1:** Conducted analysis on all columns using PyForest for automatic feature selection.\n   - **Path 2:** Removed columns with low PCA variance to reduce complexity, resulting in a similar model performance but with reduced time complexity.\n\n\n4. **Model Comparison**\n\n\n### Path 1: All Columns\n| Model                               | Adjusted R-Squared | R-Squared | RMSE      | Time Taken (s) |\n|-------------------------------------|--------------------|-----------|-----------|-----------------|\n| HistGradientBoostingRegressor       | 0.88               | 0.88      | 128699.89 | 0.77            |\n| XGBRegressor                        | 0.88               | 0.88      | 131244.14 | 0.78            |\n| LGBMRegressor                       | 0.88               | 0.88      | 131667.67 | 0.68            |\n| ... (Top models)                    | ...                | ...       | ...       | ...             |\n\n### Path 2: Reduced Columns\n| Model                               | Adjusted R-Squared | R-Squared | RMSE      | Time Taken (s) |\n|-------------------------------------|--------------------|-----------|-----------|-----------------|\n| HistGradientBoostingRegressor       | 0.88               | 0.88      | 129600.41 | 0.71            |\n| LGBMRegressor                       | 0.87               | 0.87      | 134150.78 | 0.39            |\n| XGBRegressor                        | 0.87               | 0.87      | 136435.61 | 0.55            |\n| ... (Top models)                    | ...                | ...       | ...       | ...             |\n\n\n5. ***Conclusion***\n\n- The models were evaluated using LazyRegressor to compare their performance on two different paths.\n- Path 1: All columns included.\n- Path 2: Removed some columns with low PCA to reduce complexity.\n\n- **Top 5 Models (Path 1):**\n    1. HistGradientBoostingRegressor\n    2. XGBRegressor\n    3. LGBMRegressor\n    4. GradientBoostingRegressor\n    5. RandomForestRegressor\n\n- **Top 5 Models (Path 2):**\n    1. HistGradientBoostingRegressor\n    2. LGBMRegressor\n    3. XGBRegressor\n    4. RandomForestRegressor\n    5. BaggingRegressor\n    \n    \n   **Key Findings:**\n   - The model performances between Path 1 and Path 2 are comparable.\n   - Path 2, with reduced columns, exhibits reduced time complexity (In some models).","metadata":{}},{"cell_type":"markdown","source":"////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyforest","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:40:24.902119Z","iopub.execute_input":"2024-02-29T08:40:24.902509Z","iopub.status.idle":"2024-02-29T08:40:42.087565Z","shell.execute_reply.started":"2024-02-29T08:40:24.902480Z","shell.execute_reply":"2024-02-29T08:40:42.086454Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Collecting pyforest\n  Downloading pyforest-1.1.0.tar.gz (15 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: pyforest\n  Building wheel for pyforest (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyforest: filename=pyforest-1.1.0-py2.py3-none-any.whl size=14606 sha256=277190782efa915cab403ef6fe68fd1796a5e80a40707807c082bb524759f29e\n  Stored in directory: /root/.cache/pip/wheels/9e/7d/2c/5d2f5e62de376c386fd3bf5a8e5bd119ace6a9f48f49df6017\nSuccessfully built pyforest\nInstalling collected packages: pyforest\nSuccessfully installed pyforest-1.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install lazypredict","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:40:46.877328Z","iopub.execute_input":"2024-02-29T08:40:46.877718Z","iopub.status.idle":"2024-02-29T08:40:59.495164Z","shell.execute_reply.started":"2024-02-29T08:40:46.877687Z","shell.execute_reply":"2024-02-29T08:40:59.493469Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Collecting lazypredict\n  Downloading lazypredict-0.2.12-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from lazypredict) (8.1.7)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from lazypredict) (1.2.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from lazypredict) (2.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from lazypredict) (4.66.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from lazypredict) (1.3.2)\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (from lazypredict) (4.2.0)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (from lazypredict) (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm->lazypredict) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm->lazypredict) (1.11.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->lazypredict) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->lazypredict) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->lazypredict) (2023.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->lazypredict) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\nDownloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\nInstalling collected packages: lazypredict\nSuccessfully installed lazypredict-0.2.12\n","output_type":"stream"}]},{"cell_type":"code","source":"# Libraries for PCA\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing important libraries\nimport pyforest\nfrom lazypredict.Supervised import LazyRegressor\nfrom pandas.plotting import scatter_matrix\n\n# Scikit-learn packages\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Hide warnings\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:40:59.497457Z","iopub.execute_input":"2024-02-29T08:40:59.497847Z","iopub.status.idle":"2024-02-29T08:41:01.892451Z","shell.execute_reply.started":"2024-02-29T08:40:59.497815Z","shell.execute_reply":"2024-02-29T08:41:01.890957Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Setting up max columns displayes to 100\npd.options.display.max_columns = None\npd.set_option('display.width', None)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:41:01.893839Z","iopub.execute_input":"2024-02-29T08:41:01.894407Z","iopub.status.idle":"2024-02-29T08:41:01.900403Z","shell.execute_reply.started":"2024-02-29T08:41:01.894373Z","shell.execute_reply":"2024-02-29T08:41:01.899026Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:41:01.903175Z","iopub.execute_input":"2024-02-29T08:41:01.903553Z","iopub.status.idle":"2024-02-29T08:41:01.917006Z","shell.execute_reply.started":"2024-02-29T08:41:01.903525Z","shell.execute_reply":"2024-02-29T08:41:01.915875Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Data Import and Cleaning:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/house-price-dataset-of-india/House Price India.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:50.048522Z","iopub.execute_input":"2024-02-29T09:06:50.049044Z","iopub.status.idle":"2024-02-29T09:06:50.104509Z","shell.execute_reply.started":"2024-02-29T09:06:50.049010Z","shell.execute_reply":"2024-02-29T09:06:50.103346Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:50.311873Z","iopub.execute_input":"2024-02-29T09:06:50.312275Z","iopub.status.idle":"2024-02-29T09:06:50.326427Z","shell.execute_reply.started":"2024-02-29T09:06:50.312246Z","shell.execute_reply":"2024-02-29T09:06:50.324881Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 14620 entries, 0 to 14619\nData columns (total 23 columns):\n #   Column                                 Non-Null Count  Dtype  \n---  ------                                 --------------  -----  \n 0   id                                     14620 non-null  int64  \n 1   Date                                   14620 non-null  int64  \n 2   number of bedrooms                     14620 non-null  int64  \n 3   number of bathrooms                    14620 non-null  float64\n 4   living area                            14620 non-null  int64  \n 5   lot area                               14620 non-null  int64  \n 6   number of floors                       14620 non-null  float64\n 7   waterfront present                     14620 non-null  int64  \n 8   number of views                        14620 non-null  int64  \n 9   condition of the house                 14620 non-null  int64  \n 10  grade of the house                     14620 non-null  int64  \n 11  Area of the house(excluding basement)  14620 non-null  int64  \n 12  Area of the basement                   14620 non-null  int64  \n 13  Built Year                             14620 non-null  int64  \n 14  Renovation Year                        14620 non-null  int64  \n 15  Postal Code                            14620 non-null  int64  \n 16  Lattitude                              14620 non-null  float64\n 17  Longitude                              14620 non-null  float64\n 18  living_area_renov                      14620 non-null  int64  \n 19  lot_area_renov                         14620 non-null  int64  \n 20  Number of schools nearby               14620 non-null  int64  \n 21  Distance from the airport              14620 non-null  int64  \n 22  Price                                  14620 non-null  int64  \ndtypes: float64(4), int64(19)\nmemory usage: 2.6 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df['number of bathrooms'] = df['number of bathrooms'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:50.386433Z","iopub.execute_input":"2024-02-29T09:06:50.389338Z","iopub.status.idle":"2024-02-29T09:06:50.394577Z","shell.execute_reply.started":"2024-02-29T09:06:50.389295Z","shell.execute_reply":"2024-02-29T09:06:50.393685Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns = ['id'])","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:50.678146Z","iopub.execute_input":"2024-02-29T09:06:50.678964Z","iopub.status.idle":"2024-02-29T09:06:50.687267Z","shell.execute_reply.started":"2024-02-29T09:06:50.678917Z","shell.execute_reply":"2024-02-29T09:06:50.686089Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"# PCA on Numeric Columns:","metadata":{}},{"cell_type":"code","source":"# Assuming df_numeric contains the numerical features including the 'Price' column\nfeatures_for_pca = df.drop(['Price','Date'], axis=1)\n\n# Standardize the features\nscaler = StandardScaler()\nfeatures_standardized = scaler.fit_transform(features_for_pca)\n\n# Perform PCA\npca = PCA()\npca.fit(features_standardized)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Create a DataFrame to display column names and their explained variance\ndf_explained_variance = pd.DataFrame({'Feature': features_for_pca.columns, 'Explained Variance': explained_variance_ratio})\n\n# Display the DataFrame sorted by explained variance\ndf_explained_variance_sorted = df_explained_variance.sort_values(by='Explained Variance', ascending=False)\nprint(df_explained_variance_sorted)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:50.900036Z","iopub.execute_input":"2024-02-29T09:06:50.900763Z","iopub.status.idle":"2024-02-29T09:06:50.976189Z","shell.execute_reply.started":"2024-02-29T09:06:50.900731Z","shell.execute_reply":"2024-02-29T09:06:50.974603Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"                                  Feature  Explained Variance\n0                      number of bedrooms                0.25\n1                     number of bathrooms                0.10\n2                             living area                0.09\n3                                lot area                0.07\n4                        number of floors                0.06\n5                      waterfront present                0.05\n6                         number of views                0.05\n7                  condition of the house                0.05\n8                      grade of the house                0.04\n9   Area of the house(excluding basement)                0.04\n10                   Area of the basement                0.03\n11                             Built Year                0.03\n12                        Renovation Year                0.03\n13                            Postal Code                0.02\n14                              Lattitude                0.02\n15                              Longitude                0.02\n16                      living_area_renov                0.01\n17                         lot_area_renov                0.01\n18               Number of schools nearby                0.01\n19              Distance from the airport                0.00\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Two Experimental Paths:","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:51.236458Z","iopub.execute_input":"2024-02-29T09:06:51.237223Z","iopub.status.idle":"2024-02-29T09:06:51.244191Z","shell.execute_reply.started":"2024-02-29T09:06:51.237187Z","shell.execute_reply":"2024-02-29T09:06:51.243005Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"(14620, 22)"},"metadata":{}}]},{"cell_type":"code","source":"cols_to_remove = ['living_area_renov', 'lot_area_renov', 'Number of schools nearby', 'Distance from the airport']\n\ncols_to_remove = [col for col in cols_to_remove if col in df.columns]\n\ndf2 = df.drop(cols_to_remove, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:51.410070Z","iopub.execute_input":"2024-02-29T09:06:51.411360Z","iopub.status.idle":"2024-02-29T09:06:51.417483Z","shell.execute_reply.started":"2024-02-29T09:06:51.411313Z","shell.execute_reply":"2024-02-29T09:06:51.416216Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"df2.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:51.618943Z","iopub.execute_input":"2024-02-29T09:06:51.619633Z","iopub.status.idle":"2024-02-29T09:06:51.627161Z","shell.execute_reply.started":"2024-02-29T09:06:51.619594Z","shell.execute_reply":"2024-02-29T09:06:51.626079Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"(14620, 18)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Experiment 1:","metadata":{}},{"cell_type":"code","source":"df.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:51.958680Z","iopub.execute_input":"2024-02-29T09:06:51.959135Z","iopub.status.idle":"2024-02-29T09:06:51.968017Z","shell.execute_reply.started":"2024-02-29T09:06:51.959100Z","shell.execute_reply":"2024-02-29T09:06:51.966904Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=['Price'])\ny = df.Price","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:52.137244Z","iopub.execute_input":"2024-02-29T09:06:52.137624Z","iopub.status.idle":"2024-02-29T09:06:52.144525Z","shell.execute_reply.started":"2024-02-29T09:06:52.137596Z","shell.execute_reply":"2024-02-29T09:06:52.143283Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5 ,test_size=0.25)\n\n# Checking if the training set was correcly splitted\nprint(\"Training set - Features: \", X_train.shape, \"Target: \", y_train.shape)\nprint(\"Test set - Features: \", X_test.shape, \"Target: \",y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:52.307169Z","iopub.execute_input":"2024-02-29T09:06:52.308056Z","iopub.status.idle":"2024-02-29T09:06:52.324939Z","shell.execute_reply.started":"2024-02-29T09:06:52.308018Z","shell.execute_reply":"2024-02-29T09:06:52.323586Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.model_selection import train_test_split'); }\n    "},"metadata":{}},{"name":"stdout","text":"Training set - Features:  (10965, 21) Target:  (10965,)\nTest set - Features:  (3655, 21) Target:  (3655,)\n","output_type":"stream"}]},{"cell_type":"code","source":"reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\nmodels1, predictions1 = reg.fit(X_train, X_test, y_train, y_test)\n\nprint(models1)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:06:52.479508Z","iopub.execute_input":"2024-02-29T09:06:52.479964Z","iopub.status.idle":"2024-02-29T09:09:22.125694Z","shell.execute_reply.started":"2024-02-29T09:06:52.479929Z","shell.execute_reply":"2024-02-29T09:09:22.124399Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stderr","text":" 74%|███████▍  | 31/42 [02:04<00:30,  2.81s/it]","output_type":"stream"},{"name":"stdout","text":"QuantileRegressor model failed to execute\nSolver interior-point is not anymore available in SciPy >= 1.11.0.\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 41/42 [02:29<00:01,  1.98s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2526\n[LightGBM] [Info] Number of data points in the train set: 10965, number of used features: 21\n[LightGBM] [Info] Start training from score 538441.563429\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [02:29<00:00,  3.56s/it]","output_type":"stream"},{"name":"stdout","text":"                               Adjusted R-Squared  R-Squared      RMSE  \\\nModel                                                                    \nHistGradientBoostingRegressor                0.88       0.88 128699.89   \nXGBRegressor                                 0.88       0.88 131244.14   \nLGBMRegressor                                0.88       0.88 131667.67   \nGradientBoostingRegressor                    0.87       0.87 137953.07   \nRandomForestRegressor                        0.86       0.86 139228.24   \nExtraTreesRegressor                          0.86       0.86 139336.93   \nBaggingRegressor                             0.85       0.85 144861.12   \nKNeighborsRegressor                          0.75       0.75 189220.77   \nTransformedTargetRegressor                   0.70       0.70 206956.71   \nLinearRegression                             0.70       0.70 206956.71   \nLars                                         0.70       0.70 206956.71   \nLasso                                        0.70       0.70 206956.78   \nLassoLars                                    0.70       0.70 206956.83   \nRidge                                        0.70       0.70 206957.79   \nRidgeCV                                      0.70       0.70 206967.54   \nBayesianRidge                                0.70       0.70 206975.97   \nLassoCV                                      0.70       0.70 207114.72   \nLassoLarsIC                                  0.70       0.70 207141.38   \nLassoLarsCV                                  0.70       0.70 207143.28   \nLarsCV                                       0.70       0.70 207143.28   \nSGDRegressor                                 0.69       0.69 208420.69   \nExtraTreeRegressor                           0.69       0.69 208935.02   \nDecisionTreeRegressor                        0.69       0.69 209461.14   \nElasticNet                                   0.66       0.66 218313.58   \nOrthogonalMatchingPursuitCV                  0.65       0.66 221382.13   \nHuberRegressor                               0.64       0.64 224830.83   \nPassiveAggressiveRegressor                   0.63       0.64 227754.34   \nTweedieRegressor                             0.63       0.63 229847.50   \nPoissonRegressor                             0.61       0.62 233564.20   \nGammaRegressor                               0.58       0.58 243789.19   \nOrthogonalMatchingPursuit                    0.57       0.58 245349.09   \nRANSACRegressor                              0.34       0.35 305115.25   \nAdaBoostRegressor                            0.27       0.28 320726.83   \nElasticNetCV                                 0.01       0.02 373448.23   \nDummyRegressor                              -0.01      -0.00 377052.24   \nNuSVR                                       -0.03      -0.02 381551.75   \nSVR                                         -0.06      -0.06 387514.10   \nGaussianProcessRegressor                    -0.55      -0.54 468572.80   \nKernelRidge                                 -1.38      -1.37 579957.69   \nMLPRegressor                                -1.48      -1.46 591734.37   \nLinearSVR                                   -1.99      -1.97 649977.22   \n\n                               Time Taken  \nModel                                      \nHistGradientBoostingRegressor        0.78  \nXGBRegressor                         0.67  \nLGBMRegressor                        0.49  \nGradientBoostingRegressor            3.40  \nRandomForestRegressor               11.95  \nExtraTreesRegressor                  5.72  \nBaggingRegressor                     1.22  \nKNeighborsRegressor                  0.30  \nTransformedTargetRegressor           0.10  \nLinearRegression                     0.18  \nLars                                 0.10  \nLasso                                0.99  \nLassoLars                            0.13  \nRidge                                0.07  \nRidgeCV                              0.19  \nBayesianRidge                        0.15  \nLassoCV                              0.66  \nLassoLarsIC                          0.19  \nLassoLarsCV                          0.26  \nLarsCV                               0.27  \nSGDRegressor                         0.14  \nExtraTreeRegressor                   0.16  \nDecisionTreeRegressor                0.28  \nElasticNet                           0.11  \nOrthogonalMatchingPursuitCV          0.20  \nHuberRegressor                       0.32  \nPassiveAggressiveRegressor           0.63  \nTweedieRegressor                     0.16  \nPoissonRegressor                     0.14  \nGammaRegressor                       0.08  \nOrthogonalMatchingPursuit            0.07  \nRANSACRegressor                      1.07  \nAdaBoostRegressor                    1.26  \nElasticNetCV                         0.59  \nDummyRegressor                       0.02  \nNuSVR                                6.47  \nSVR                                  9.75  \nGaussianProcessRegressor            51.64  \nKernelRidge                         17.15  \nMLPRegressor                        31.22  \nLinearSVR                            0.13  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- **Top 5 Models (Path 1):**\n    1. HistGradientBoostingRegressor\n    2. XGBRegressor\n    3. LGBMRegressor\n    4. GradientBoostingRegressor\n    5. RandomForestRegressor","metadata":{}},{"cell_type":"markdown","source":"## Experiment 2:","metadata":{}},{"cell_type":"code","source":"df2.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf2.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:09:22.128088Z","iopub.execute_input":"2024-02-29T09:09:22.128865Z","iopub.status.idle":"2024-02-29T09:09:22.138212Z","shell.execute_reply.started":"2024-02-29T09:09:22.128823Z","shell.execute_reply":"2024-02-29T09:09:22.137150Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"A = df2.drop(columns=['Price'])\nb = df2.Price","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:09:22.139732Z","iopub.execute_input":"2024-02-29T09:09:22.140111Z","iopub.status.idle":"2024-02-29T09:09:22.148825Z","shell.execute_reply.started":"2024-02-29T09:09:22.140083Z","shell.execute_reply":"2024-02-29T09:09:22.147860Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"A_train, A_test, b_train, b_test = train_test_split(A, b, random_state=5, test_size=0.25)\n\n# Checking if the training set was correcly splitted\nprint(\"Training set - Features: \", A_train.shape, \"Target: \", b_train.shape)\nprint(\"Test set - Features: \", A_test.shape, \"Target: \",b_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:09:22.150719Z","iopub.execute_input":"2024-02-29T09:09:22.151055Z","iopub.status.idle":"2024-02-29T09:09:22.166397Z","shell.execute_reply.started":"2024-02-29T09:09:22.151023Z","shell.execute_reply":"2024-02-29T09:09:22.164989Z"},"trusted":true},"execution_count":73,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.model_selection import train_test_split'); }\n    "},"metadata":{}},{"name":"stdout","text":"Training set - Features:  (10965, 17) Target:  (10965,)\nTest set - Features:  (3655, 17) Target:  (3655,)\n","output_type":"stream"}]},{"cell_type":"code","source":"reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\nmodels2 , predictions2 = reg.fit(A_train, A_test, b_train, b_test)\n\nprint(models2)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:09:22.168107Z","iopub.execute_input":"2024-02-29T09:09:22.168432Z","iopub.status.idle":"2024-02-29T09:11:44.423783Z","shell.execute_reply.started":"2024-02-29T09:09:22.168407Z","shell.execute_reply":"2024-02-29T09:11:44.422581Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stderr","text":" 74%|███████▍  | 31/42 [02:01<00:30,  2.79s/it]","output_type":"stream"},{"name":"stdout","text":"QuantileRegressor model failed to execute\nSolver interior-point is not anymore available in SciPy >= 1.11.0.\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 41/42 [02:21<00:01,  1.72s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003452 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1980\n[LightGBM] [Info] Number of data points in the train set: 10965, number of used features: 17\n[LightGBM] [Info] Start training from score 538441.563429\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [02:22<00:00,  3.39s/it]","output_type":"stream"},{"name":"stdout","text":"                               Adjusted R-Squared  R-Squared      RMSE  \\\nModel                                                                    \nHistGradientBoostingRegressor                0.88       0.88 129600.41   \nLGBMRegressor                                0.87       0.87 134150.78   \nXGBRegressor                                 0.87       0.87 136435.61   \nRandomForestRegressor                        0.87       0.87 137128.54   \nBaggingRegressor                             0.86       0.86 140333.88   \nGradientBoostingRegressor                    0.86       0.86 142018.26   \nExtraTreesRegressor                          0.86       0.86 143215.99   \nKNeighborsRegressor                          0.75       0.75 188210.73   \nTransformedTargetRegressor                   0.70       0.70 206952.68   \nLinearRegression                             0.70       0.70 206952.68   \nLassoLarsIC                                  0.70       0.70 206952.68   \nLars                                         0.70       0.70 206952.68   \nLasso                                        0.70       0.70 206952.86   \nLassoLars                                    0.70       0.70 206952.86   \nRidge                                        0.70       0.70 206953.57   \nRidgeCV                                      0.70       0.70 206961.74   \nBayesianRidge                                0.70       0.70 206965.02   \nLassoCV                                      0.70       0.70 207004.12   \nLarsCV                                       0.70       0.70 207014.75   \nLassoLarsCV                                  0.70       0.70 207014.75   \nDecisionTreeRegressor                        0.70       0.70 207211.45   \nSGDRegressor                                 0.69       0.69 208272.97   \nElasticNet                                   0.66       0.66 219061.97   \nExtraTreeRegressor                           0.66       0.66 219264.70   \nOrthogonalMatchingPursuitCV                  0.65       0.66 221382.13   \nHuberRegressor                               0.65       0.65 223821.53   \nPassiveAggressiveRegressor                   0.64       0.64 226184.84   \nTweedieRegressor                             0.62       0.62 232078.52   \nPoissonRegressor                             0.57       0.57 247179.44   \nGammaRegressor                               0.54       0.54 256170.73   \nOrthogonalMatchingPursuit                    0.51       0.51 263269.41   \nRANSACRegressor                              0.35       0.35 303509.98   \nAdaBoostRegressor                            0.33       0.33 308706.37   \nElasticNetCV                                 0.01       0.02 373928.04   \nDummyRegressor                              -0.00      -0.00 377052.24   \nNuSVR                                       -0.03      -0.02 381533.04   \nSVR                                         -0.06      -0.06 387507.69   \nGaussianProcessRegressor                    -0.11      -0.11 396659.15   \nKernelRidge                                 -1.38      -1.37 580478.57   \nMLPRegressor                                -1.53      -1.52 598870.34   \nLinearSVR                                   -1.99      -1.97 649977.22   \n\n                               Time Taken  \nModel                                      \nHistGradientBoostingRegressor        0.71  \nLGBMRegressor                        0.40  \nXGBRegressor                         0.57  \nRandomForestRegressor                9.19  \nBaggingRegressor                     0.96  \nGradientBoostingRegressor            2.51  \nExtraTreesRegressor                  4.43  \nKNeighborsRegressor                  0.29  \nTransformedTargetRegressor           0.09  \nLinearRegression                     0.18  \nLassoLarsIC                          0.17  \nLars                                 0.09  \nLasso                                0.81  \nLassoLars                            0.12  \nRidge                                0.06  \nRidgeCV                              0.17  \nBayesianRidge                        0.14  \nLassoCV                              0.77  \nLarsCV                               0.23  \nLassoLarsCV                          0.24  \nDecisionTreeRegressor                0.24  \nSGDRegressor                         0.14  \nElasticNet                           0.10  \nExtraTreeRegressor                   0.15  \nOrthogonalMatchingPursuitCV          0.18  \nHuberRegressor                       0.27  \nPassiveAggressiveRegressor           0.55  \nTweedieRegressor                     0.15  \nPoissonRegressor                     0.13  \nGammaRegressor                       0.09  \nOrthogonalMatchingPursuit            0.07  \nRANSACRegressor                      1.02  \nAdaBoostRegressor                    1.05  \nElasticNetCV                         0.56  \nDummyRegressor                       0.02  \nNuSVR                                6.11  \nSVR                                  8.60  \nGaussianProcessRegressor            51.43  \nKernelRidge                         17.35  \nMLPRegressor                        31.51  \nLinearSVR                            0.13  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- **Top 5 Models (Path 2):**\n    1. HistGradientBoostingRegressor\n    2. LGBMRegressor\n    3. XGBRegressor\n    4. RandomForestRegressor\n    5. BaggingRegressor","metadata":{}},{"cell_type":"markdown","source":"### Experiment Significance:\n\nComparing the results from the two experiments (with and without column reduction), it appears that the choice of columns did not significantly impact the performance of the `HistGradientBoostingRegressor` on this dataset. Both experiments yielded comparable RMSE values, suggesting that the model's robustness to feature variations.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}